{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2e402e87eab1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mZTF_CDA_Query\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdb_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnearMOC_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconesearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Packages for direct database access\n",
    "# %pip install psycopg2\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "# Packages for data and number handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Packages for calculating current time and extracting ZTF data to VOTable\n",
    "from astropy.time import Time\n",
    "from astropy.table import Table, Column, unique, vstack\n",
    "from astropy.io.votable import parse_single_table, from_table, writeto\n",
    "from datetime import datetime\n",
    "\n",
    "# Package for nearMOC filtering\n",
    "from pystilts import tpipe\n",
    "\n",
    "# Package for coordinates\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "\n",
    "# Virtual observatory package\n",
    "import pyvo as vo\n",
    "\n",
    "# Package for footprint geometric descriptions\n",
    "import shapely.geometry as sg\n",
    "import shapely.ops as so\n",
    "\n",
    "# Packages for display and data plotting, if desired\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from .ZTF_CDA_Query import db_query, nearMOC_filter, conesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = 'lightcurve'\n",
    "days_ago = 2.0\n",
    "filename = 'ztf_api_lc_objects_unique.xml'\n",
    "outfile = 'ztf_CXC_footprint_LC.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    days_ago = float(i)\n",
    "    \n",
    "    # Start timer\n",
    "    start = time.time()\n",
    "    \n",
    "    db_query(classifier, days_ago, filename)\n",
    "    ztfobjects = nearMOC_filter(filename, outfile)\n",
    "    conesearch(ztfobjects)\n",
    "    \n",
    "    # End timer\n",
    "    end = time.time()\n",
    "    print(f\"Runtime of the DDB query is {end - start}\")\n",
    "    print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and load database credentials\n",
    "credentials_file = \"../alercereaduser_v4.json\"\n",
    "with open(credentials_file) as jsonfile:\n",
    "    params = json.load(jsonfile)[\"params\"]\n",
    "    \n",
    "# Open a connection to the database\n",
    "conn = psycopg2.connect(dbname=params['dbname'], \n",
    "                        user=params['user'], \n",
    "                        host=params['host'], \n",
    "                        password=params['password'])\n",
    "\n",
    "days_ago = 2.0\n",
    "min_lastmjd = np.floor(Time(datetime.today(), scale='utc').mjd) - (days_ago + 1.0)\n",
    "max_lastmjd = np.floor(Time(datetime.today(), scale='utc').mjd) - days_ago\n",
    "\n",
    "classifiers = {'stamp': [\"\\'stamp_classifier\\'\", \"\\'stamp_classifier_1.0.4\\'\"], \n",
    "               'lightcurve': [\"\\'lc_classifier\\'\", \"\\'hierarchical_random_forest_1.0.0\\'\"]}\n",
    "\n",
    "classifier = 'lightcurve'\n",
    "classifier_name = classifiers[classifier][0]\n",
    "classifier_version = classifiers[classifier][1]\n",
    "\n",
    "query='''\n",
    "SELECT\n",
    "    object.oid, object.meanra, object.meandec, object.sigmara, object.sigmadec,\n",
    "    object.firstmjd, object.lastmjd, object.ndet, \n",
    "    pr.classifier_name, pr.classifier_version, pr.class_name, \n",
    "    pr.ranking, pr.probability\n",
    "\n",
    "FROM \n",
    "    object INNER JOIN (\n",
    "        SELECT \n",
    "            probability.oid, probability.classifier_name, probability.classifier_version,\n",
    "            probability.class_name, probability.ranking, probability.probability\n",
    "        FROM\n",
    "            probability\n",
    "        WHERE\n",
    "            probability.classifier_name = %s\n",
    "            AND probability.classifier_version = %s\n",
    "            AND probability.ranking = 1\n",
    "    ) AS pr\n",
    "    ON object.oid = pr.oid\n",
    "\n",
    "WHERE \n",
    "    object.lastMJD >= %s\n",
    "    AND object.lastMJD <= %s\n",
    "''' % (classifier_name, classifier_version, str(min_lastmjd), str(max_lastmjd))\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Outputs as a pd.DataFrame\n",
    "dbobjects = pd.read_sql_query(query, conn)\n",
    "\n",
    "# End timer\n",
    "end = time.time()\n",
    "print(f\"Runtime of the DDB query is {end - start}\")\n",
    "\n",
    "# Sorting detections by lastMJD, firstMJD, and OID in descending order\n",
    "apiobjects = dbobjects.sort_values(by=['lastmjd', 'firstmjd', 'oid'], ascending=False)\n",
    "\n",
    "# Count number of OIDs that correspond to each class name\n",
    "print('Total rows : %i' % (len(apiobjects.index)))\n",
    "obj_classes = apiobjects.groupby('class_name')\n",
    "for key in obj_classes.groups.keys():\n",
    "    l = obj_classes.groups[key].size\n",
    "    print('%s : %i' % (key, l))\n",
    "\n",
    "# Identify duplicate OID entries - rows with same OID but different classes and probabilities\n",
    "obj_oid = apiobjects.groupby(['oid'])\n",
    "duplicates = []\n",
    "for key in obj_oid.groups.keys():\n",
    "    l = obj_oid.groups[key].size\n",
    "    if l > 1:\n",
    "        oid = key\n",
    "        duplicates.append(oid)\n",
    "\n",
    "print('\\n ---------------- \\n')\n",
    "print('Number of OIDs with more than one row : %i' % (len(duplicates)))\n",
    "print('Number of unique OIDs : %i' % len(obj_oid))\n",
    "\n",
    "# Defining a function that allows you to export the dataframe into a VOTable\n",
    "def export_object_data(objects_df, filename):\n",
    "    # Filling the masked values with the string 'NaN'\n",
    "    objects_filled = objects_df.fillna('None')\n",
    "\n",
    "    # Converting filled dataframe to astropy Table, then astropy VOTableFile, then exporting into .xml\n",
    "    full_dt = Table.from_pandas(objects_filled)\n",
    "    votable = from_table(full_dt)\n",
    "    writeto(votable, filename)\n",
    "\n",
    "# Export VOTable with only unique OIDs\n",
    "unique_df = apiobjects.drop_duplicates(subset=['oid'])\n",
    "filename = 'ztf_api_lc_objects_unique.xml'\n",
    "export_object_data(unique_df, filename)\n",
    "\n",
    "# Running pystilts\n",
    "mocLocation='/Users/cxc/CDAAnnotation/FITS_handling/ChandraMOC13_nograting.fits'\n",
    "infile='ztf_api_lc_objects_unique.xml'\n",
    "outfile='ztf_CXC_footprint_LC.xml'\n",
    "expression=\"\"\"nearMoc(\\\\\"%s\\\\\", meanra, meandec, 0.02)\"\"\" % mocLocation\n",
    "expression='\"{0}\"'.format(expression)\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "tpipe(infile=infile,\n",
    "      ifmt='votable',\n",
    "      ofmt='votable',\n",
    "      omode='out',\n",
    "      outfile=outfile, cmd='select %s'%expression)\n",
    "\n",
    "# End timer\n",
    "end = time.time()\n",
    "print(f\"Runtime of the pystils program is {end - start}\")\n",
    "\n",
    "ztfobjects = parse_single_table(outfile).to_table()\n",
    "print('Number of ZTF obj filtered by nearMOC: %i' % len(np.unique(ztfobjects['oid'].filled())))\n",
    "\n",
    "skycoords = SkyCoord(ra=ztfobjects['meanra']*u.degree, dec=ztfobjects['meandec']*u.degree, frame='icrs')\n",
    "# cone = vo.dal.SCSService('http://cda.cfa.harvard.edu/csc2scs/coneSearch')\n",
    "cone = vo.dal.SCSService('https://cda.cfa.harvard.edu/cxcscs/coneSearch')\n",
    "maxrad = 50.0 * u.arcmin\n",
    "\n",
    "csc_results = []\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "for i in trange(len(ztfobjects), desc='Indexing'):\n",
    "    results = cone.search(pos=skycoords[i], radius=maxrad)\n",
    "    csc_results.append(results.to_table().as_array().data)\n",
    "\n",
    "# End timer\n",
    "end = time.time()\n",
    "print(f\"Runtime of the cone search program is {end - start}\")\n",
    "\n",
    "csc_res = np.asarray(csc_results, dtype=object)\n",
    "concsc_res = np.concatenate(csc_res)\n",
    "obsids = set(concsc_res['obsid'].astype(str))\n",
    "tobsids = tuple(obsids)\n",
    "\n",
    "tapservice = vo.dal.TAPService(\"https://cda.cfa.harvard.edu/cxctap/\")\n",
    "query = '''\n",
    "SELECT o.obs_id, o.obs_creation_date, o.s_ra, o.s_dec, o.s_region\n",
    "FROM ivoa.ObsCore AS o\n",
    "WHERE o.dataproduct_type = 'event' AND o.obs_id IN {}\n",
    "'''.format(tobsids)\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "tapresult = tapservice.search(query)\n",
    "\n",
    "# End timer\n",
    "end = time.time()\n",
    "print(f\"Runtime of the tapservice query is {end - start}\")\n",
    "\n",
    "tresult = tapresult.to_table()\n",
    "tresult['obs_creation_date'] = Time(tresult['obs_creation_date'].astype(str), format='isot', scale='utc').mjd\n",
    "tresult['obs_id'] = tresult['obs_id'].astype(object)\n",
    "tresult['s_ra'] = tresult['s_ra'].astype(object)\n",
    "tresult['s_dec'] = tresult['s_dec'].astype(object)\n",
    "tresult['s_region'] = tresult['s_region'].astype(object)\n",
    "tresult = unique(tresult, keys='obs_id')\n",
    "tresult.add_index(['obs_id'])\n",
    "\n",
    "f = np.vectorize(np.size)\n",
    "n_res = f(csc_res)\n",
    "ztfobjects['n_res']=n_res\n",
    "mask = (ztfobjects['n_res'] > 0)\n",
    "maskedztf = ztfobjects[mask]\n",
    "l = len(concsc_res)\n",
    "\n",
    "#Initializing columns\n",
    "maskedztf['cxc_obs_id'] = np.full(len(maskedztf), None)\n",
    "maskedztf['cxc_obs_status'] = np.full(len(maskedztf), None)\n",
    "maskedztf['cxc_obs_creation_date'] = np.full(len(maskedztf), None)\n",
    "maskedztf['cxc_s_ra'] = np.full(len(maskedztf), None)\n",
    "maskedztf['cxc_s_dec'] = np.full(len(maskedztf), None)\n",
    "maskedztf['cxc_s_region'] = np.full(len(maskedztf), None)\n",
    "\n",
    "mztf = maskedztf.as_array()\n",
    "mztf_f = mztf.filled()\n",
    "\n",
    "mztf_nres = n_res[n_res != 0]\n",
    "mztf_fr = np.repeat(mztf,mztf_nres)\n",
    "\n",
    "mztf_fr['cxc_obs_id'] = concsc_res['obsid']\n",
    "mztf_fr['cxc_obs_status'] = concsc_res['status']\n",
    "\n",
    "bobsids = np.array(list(set(tresult['obs_id'])))\n",
    "a = mztf_fr['cxc_obs_id']\n",
    "idx = np.where(np.in1d(a,bobsids))[0]\n",
    "tt = tresult.loc[a[idx]]\n",
    "\n",
    "for key in ['obs_creation_date','s_ra','s_dec','s_region']:\n",
    "    mztf_fr['cxc_%s'%(key)][idx] = tt[key]\n",
    "\n",
    "ztfobs = Table(mztf_fr)\n",
    "cxc_observed_statuses = ['archived', 'observed']\n",
    "mask = np.in1d(ztfobs['cxc_obs_status'].astype(str),cxc_observed_statuses)\n",
    "observedztf = ztfobs[mask]\n",
    "nonobservedztf = ztfobs[~mask]\n",
    "\n",
    "print('Total unique ZTF OIDs: %i' % len(np.unique(ztfobs['oid'].filled())))\n",
    "print('ZTF OIDs with cone search matches that have footprint: %i' % len(np.unique(observedztf['oid'].filled())))\n",
    "print('ZTF OIDs with cone search matches that do not have footprint: %i' % len(np.unique(nonobservedztf['oid'].filled())))\n",
    "\n",
    "print('Total number of archived or observed footprints: %i' % len(observedztf))\n",
    "print('Total number of scheduled, unobserved, or triggered footprints: %i' % len(nonobservedztf))\n",
    "\n",
    "ztfobs['in_poly']=np.full(len(ztfobs), None)\n",
    "ztf_in_poly=[]\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "for i in trange(len(observedztf)):\n",
    "    region = observedztf['cxc_s_region'].filled()[i].decode().replace(\")\",\"\").split('POLYGON')\n",
    "    polygon_coords = [np.stack((a.split()[::2], a.split()[1::2]), axis=-1).astype(float) for a in region[1:]]\n",
    "    \n",
    "    polygons = [sg.Polygon(c) for c in polygon_coords]\n",
    "    \n",
    "    point = sg.Point(np.asarray([observedztf['meanra'][i],observedztf['meandec'][i]]).astype(float))\n",
    "    \n",
    "    in_poly = set([point.within(poly) for poly in polygons])\n",
    "    ztf_in_poly.append((True in in_poly))\n",
    "\n",
    "ztfobs['in_poly'][mask]=np.asarray(ztf_in_poly)\n",
    "\n",
    "# End timer\n",
    "end = time.time()\n",
    "print(f\"Runtime of the footprint matching program is {end - start}\")\n",
    "\n",
    "ztf_infootprint = ztfobs[ztfobs['in_poly'] == True]\n",
    "\n",
    "print('Total cone search matches with ZTF object that falls in footprint: %i out of %i' % (len(ztf_infootprint), len(observedztf)))\n",
    "print(\"Total ZTF OIDs that fall in their matches\\' footprint: %i out of %i\" % (len(np.unique(ztf_infootprint['oid'].filled())), len(np.unique(ztfobs['oid'].filled()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observedztf = ztfobs[mask]\n",
    "nonobservedztf = ztfobs[~mask]\n",
    "obsztfoids = observedztf.group_by('oid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplots=50\n",
    "nrows=10\n",
    "ncols=5\n",
    "\n",
    "start = 0\n",
    "stop = start + nplots\n",
    "testoids = range(start, stop)\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(ncols*10,nrows*10))\n",
    "counts = np.arange(nplots).reshape((nrows, ncols))\n",
    "\n",
    "for idx, i in enumerate(testoids):\n",
    "    group = obsztfoids.groups[i]\n",
    "    group.sort('cxc_obs_id')\n",
    "    polygons = []\n",
    "    \n",
    "    ax_ind = np.concatenate(np.where(counts == idx))\n",
    "    ax = axs[ax_ind[0], ax_ind[1]]\n",
    "\n",
    "    if (len(group['cxc_s_region'])==1):\n",
    "        region = group['cxc_s_region'][0].decode()\n",
    "        s_region = region.replace(\")\",\"\").split('POLYGON')\n",
    "        polygon_coords = [np.stack((a.split()[::2], a.split()[1::2]), axis=-1).astype(float) for a in s_region[1:]]\n",
    "        polygons += [sg.Polygon(c) for c in polygon_coords]\n",
    "        new_shape = so.unary_union(polygons)\n",
    "    else:\n",
    "        for region in group['cxc_s_region'].filled():\n",
    "            s_region = region.decode().replace(\")\",\"\").split('POLYGON')\n",
    "            polygon_coords = [np.stack((a.split()[::2], a.split()[1::2]), axis=-1).astype(float) for a in s_region[1:]]\n",
    "            polygons += [sg.Polygon(c) for c in polygon_coords]\n",
    "\n",
    "    new_shape = so.unary_union(polygons)\n",
    "\n",
    "    if type(new_shape)==type(sg.MultiPolygon()):\n",
    "        for geom in new_shape.geoms:\n",
    "            ax.plot(*geom.exterior.xy, color='lightsteelblue')\n",
    "            ax.fill(*geom.exterior.xy, alpha=0.3, fc='cornflowerblue', ec='none')\n",
    "    else:\n",
    "        ax.plot(*new_shape.exterior.xy, color='lightsteelblue')\n",
    "        ax.fill(*new_shape.exterior.xy, alpha=0.3, fc='cornflowerblue', ec='none')\n",
    "\n",
    "\n",
    "    x=group['cxc_s_ra'].filled().astype(float)\n",
    "    y=group['cxc_s_dec'].filled().astype(float)\n",
    "    color_col = Column(data=[idx for idx, k in enumerate(group['cxc_obs_id'])], name='plot_colors')\n",
    "    ax.scatter(x=x,\n",
    "               y=y,\n",
    "               marker='o', alpha=0.5, c=color_col, s=50)\n",
    "   \n",
    "    a,b = np.asarray([group['meanra'], group['meandec']]).astype(float)\n",
    "    ax.scatter(a,b, marker='+', color='red', s=100)\n",
    "    ax.set_xlabel('RA (deg)')\n",
    "    ax.set_ylabel('Dec (deg)')\n",
    "    \n",
    "    #Formatting title\n",
    "    inpoly = str((True in group['in_poly']))\n",
    "    cxc_obsids = str(len(group['cxc_obs_id'].astype(str)))\n",
    "    title = group.groups.keys[0] + ', in_poly = '+ inpoly + ', # CXC obs = ' + cxc_obsids\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.savefig(\"footprint_plots_LC-TEST.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
